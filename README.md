# Accident Analytics Pipeline ‚Äî DATA226
(Airflow ‚Üí Snowflake ‚Üí dbt ‚Üí Tableau)

## üìò Overview
This project implements an ELT (Extract‚ÄìLoad‚ÄìTransform) pipeline for analyzing traffic accident data from Santa Clara County using Airflow, Snowflake, dbt, and Tableau.

Pipeline steps:
1. Extraction ‚Äî historical crash CSV, live weather API, live traffic API  
2. Loading ‚Äî write raw data into Snowflake RAW schema  
3. Transformation ‚Äî dbt models (staging ‚Üí intermediate ‚Üí marts)  
4. Visualization ‚Äî Tableau dashboards for trends, hotspots, and risk analysis  

---

## üß± Architecture Diagram (Mermaid)

    mermaid
    flowchart LR
        CSV[Historical Crash Data] --> A[traffic_crash_etl.py\nAirflow DAG]
        WEATHER[OpenWeather API] --> W[weather.py\nAirflow DAG]
        TRAFFIC[Google Distance Matrix API] --> G[Google_maps.py\nAirflow DAG]
        A --> RAW[Snowflake RAW Schema]
        W --> RAW
        G --> RAW
        RAW --> DBT[dbt Models]
        DBT --> MART[Snowflake MART Schema]
        MART --> TABLEAU[Tableau Dashboards]
        TABLEAU --> INSIGHTS[Risk Hotspots, Weather Impact, Crash Forecasts]

---

## üìÅ Repository Structure

    .
    ‚îú‚îÄ‚îÄ dags/
    ‚îÇ   ‚îú‚îÄ‚îÄ Google_maps.py           # DAG for Google Distance Matrix (traffic)
    ‚îÇ   ‚îú‚îÄ‚îÄ weather.py               # DAG for OpenWeatherMap (weather)
    ‚îÇ   ‚îú‚îÄ‚îÄ traffic_crash_etl.py     # Main crash ETL DAG (loads CSV, runs dbt)
    ‚îÇ   ‚îî‚îÄ‚îÄ snowflake_connector.py   # Shared Snowflake connection / utilities
    ‚îú‚îÄ‚îÄ data/                        # Historical accident dataset(s)
    ‚îú‚îÄ‚îÄ tableau/                     # Tableau dashboards / screenshots
    ‚îú‚îÄ‚îÄ compose.yaml                 # Docker Compose for Airflow stack
    ‚îî‚îÄ‚îÄ README.md

---

## üîß Prerequisites

- Python 3.10+  
- Docker & Docker Compose  
- Snowflake account  
- dbt-core + dbt-snowflake  
- Tableau Desktop or Tableau Public  
- API keys:
  - OpenWeatherMap  
  - Google Distance Matrix API  

---

## üîê Required Environment Variables

    export SNOWFLAKE_ACCOUNT="<account>"
    export SNOWFLAKE_USER="<user>"
    export SNOWFLAKE_PASSWORD="<password>"
    export SNOWFLAKE_ROLE="DATA226_ROLE"
    export SNOWFLAKE_WAREHOUSE="COMPUTE_WH"
    export SNOWFLAKE_DATABASE="ACCIDENT_DW"
    export SNOWFLAKE_SCHEMA="RAW"

    export OPENWEATHER_API_KEY="<weather_key>"
    export GOOGLE_DISTANCE_MATRIX_API_KEY="<maps_key>"

    export DBT_PROFILES_DIR="$(pwd)/dbt"
    export AIRFLOW_HOME="$(pwd)/.airflow"

---

## üåÄ Airflow Configuration

### 1. Start Airflow with Docker Compose

    docker-compose -f compose.yaml up --build

### 2. Airflow UI

    http://localhost:8080
    username: airflow
    password: airflow

### 3. Snowflake Connection (snowflake_conn)

    Conn Type: Snowflake
    Account: <account>
    User: <user>
    Password: <password>
    Warehouse: COMPUTE_WH
    Database: ACCIDENT_DW
    Schema: RAW
    Role: DATA226_ROLE

### 4. Airflow Variables

    snowflake_database      = ACCIDENT_DW
    raw_schema              = RAW
    intermediate_schema     = INT
    mart_schema             = MART
    openweather_api_key     = <key>
    traffic_api_key         = <key>

---

## üì° DAGs (by file)

### Google_maps.py
- Airflow DAG to call **Google Distance Matrix API**
- Fetches travel time / congestion for configured origin‚Äìdestination pairs
- Writes raw traffic data into `RAW.TRAFFIC_*` tables in Snowflake

### weather.py
- Airflow DAG to call **OpenWeatherMap API**
- Fetches current weather for relevant locations / time ranges
- Writes raw weather data into `RAW.WEATHER_*` tables in Snowflake

### traffic_crash_etl.py
- Main **crash ETL DAG**
- Reads crash CSV files from `data/`
- Uses `snowflake_connector.py` to load into `RAW.CRASHES`
- Triggers dbt (staging ‚Üí intermediate ‚Üí marts) once loads succeed

### snowflake_connector.py
- Shared utility module used by the DAGs
- Manages Snowflake connections, queries, and table creation
- Encapsulates common DDL/DML used by ETL tasks

---

## üß± dbt Layer

Example manual dbt commands (inside your dbt project):

    cd dbt
    dbt debug
    dbt run
    dbt test

Example checks in Snowflake:

    SELECT COUNT(*) FROM RAW.CRASHES;
    SELECT * FROM MART.FACT_CRASHES ORDER BY CRASH_DATE DESC LIMIT 20;

dbt models typically include:

- Staging models: cleaned versions of `RAW` tables  
- Intermediate models: crash joined with weather + traffic  
- Mart models:  
    - `FACT_CRASHES`  
    - `DIM_DATE`  
    - `DIM_LOCATION`  
    - `DIM_WEATHER`  
    - `DIM_TRAFFIC`  

---

## üìä Tableau Dashboard

Snowflake connection settings for Tableau:

    Warehouse: COMPUTE_WH
    Database: ACCIDENT_DW
    Schema: MART

Recommended charts:

- Crashes by month / year  
- Severity distribution (minor, moderate, severe, fatal)  
- Collision type breakdown  
- Weather vs. traffic control heatmap  
- Road surface and lighting condition impacts  
- Geospatial accident hotspots (map)  
- Simple crash forecast over time  

Combine these into a single **Accident Analytics Dashboard** for your presentation.

---

## üìÑ License

For educational use in **DATA 226 ‚Äî Data Warehousing** (San Jos√© State University).
