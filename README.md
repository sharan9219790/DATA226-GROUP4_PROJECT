# Santa Clara Crash Analytics Pipeline â€” DATA226 - Group project
(Airflow â†’ Snowflake â†’ dbt â†’ Tableau)

## ğŸ“˜ Overview
This project implements an ELT (Extractâ€“Loadâ€“Transform) pipeline for analyzing traffic accident data from Santa Clara County using Airflow, Snowflake, dbt, and Tableau.

Pipeline steps:
1. Extraction â€” historical crash CSV, live weather API, live traffic API  
2. Loading â€” write raw data into Snowflake RAW schema  
3. Transformation â€” dbt models (staging â†’ intermediate â†’ marts)  
4. Visualization â€” Tableau dashboards for trends, hotspots, and risk analysis  

---

## ğŸ“ Repository Structure

    .
    â”œâ”€â”€ dags/
    â”‚   â”œâ”€â”€ Google_maps.py           # DAG for Google Distance Matrix (traffic)
    â”‚   â”œâ”€â”€ weather.py               # DAG for OpenWeatherMap (weather)
    â”‚   â”œâ”€â”€ traffic_crash_etl.py     # Main crash ETL DAG (loads CSV, runs dbt)
    â”‚   â””â”€â”€ snowflake_connector.py   # Shared Snowflake connection / utilities
    â”œâ”€â”€ data/                        # Historical accident dataset(s)
    â”œâ”€â”€ tableau/                     # Tableau dashboards / screenshots
    â”œâ”€â”€ compose.yaml                 # Docker Compose for Airflow stack
    â””â”€â”€ README.md
    |___ dbt                         #dbt
---

## ğŸ”§ Prerequisites

- Python 3.10+  
- Docker & Docker Compose  
- Snowflake account  
- dbt-core + dbt-snowflake  
- Tableau Desktop or Tableau Public  
- API keys:
  - OpenWeatherMap  
  - Google Distance Matrix API  

---

## ğŸ” Required Environment Variables

    export SNOWFLAKE_ACCOUNT="<account>"
    export SNOWFLAKE_USER="<user>"
    export SNOWFLAKE_PASSWORD="<password>"
    export SNOWFLAKE_ROLE="ROLE"
    export SNOWFLAKE_WAREHOUSE="COMPUTE_WH"
    export SNOWFLAKE_DATABASE="ACCIDENT_DW"
    export SNOWFLAKE_SCHEMA="RAW"

    export OPENWEATHER_API_KEY="<weather_key>"
    export GOOGLE_DISTANCE_MATRIX_API_KEY="<maps_key>"

    export DBT_PROFILES_DIR="$(pwd)/dbt"
    export AIRFLOW_HOME="$(pwd)/.airflow"

---

## ğŸŒ€ Airflow Configuration

### 1. Start Airflow with Docker Compose

    docker-compose -f compose.yaml up --build

### 2. Airflow UI

    http://localhost:8080
    username: airflow
    password: airflow

### 3. Snowflake Connection (snowflake_conn)

    Conn Type: Snowflake
    Account: <account>
    User: <user>
    Password: <password>
    Warehouse: COMPUTE_WH
    Database: ACCIDENT_DW
    Schema: RAW
    Role: DATA226_ROLE

### 4. Airflow Variables

    snowflake_database      = ACCIDENT_DW
    raw_schema              = RAW
    intermediate_schema     = INT
    mart_schema             = MART
    openweather_api_key     = <key>
    traffic_api_key         = <key>

---

## ğŸ“¡ DAGs (by file)

### Google_maps.py
- Airflow DAG to call **Google Distance Matrix API**
- Fetches travel time / congestion for configured originâ€“destination pairs
- Writes raw traffic data into `RAW.TRAFFIC_*` tables in Snowflake

### weather.py
- Airflow DAG to call **OpenWeatherMap API**
- Fetches current weather for relevant locations / time ranges
- Writes raw weather data into `RAW.WEATHER_*` tables in Snowflake

### traffic_crash_etl.py
- Main **crash ETL DAG**
- Reads crash CSV files from `data/`
- Uses `snowflake_connector.py` to load into `RAW.CRASHES`
- Triggers dbt (staging â†’ intermediate â†’ marts) once loads succeed

### snowflake_connector.py
- Shared utility module used by the DAGs
- Manages Snowflake connections, queries, and table creation
- Encapsulates common DDL/DML used by ETL tasks

---

## ğŸ§± dbt Layer

Example manual dbt commands (inside your dbt project):

    cd dbt
    dbt debug
    dbt run
    dbt test

Example checks in Snowflake:

    SELECT COUNT(*) FROM RAW.CRASHES;
    SELECT * FROM MART.FACT_CRASHES ORDER BY CRASH_DATE DESC LIMIT 20;

dbt models typically include:

- Staging models: cleaned versions of `RAW` tables  
- Intermediate models: crash joined with weather + traffic  
- Mart models:  
    - `FACT_CRASHES`  
    - `DIM_DATE`  
    - `DIM_LOCATION`  
    - `DIM_WEATHER`  
    - `DIM_TRAFFIC`  

---

## ğŸ“Š Tableau Dashboard

Snowflake connection settings for Tableau:

    Warehouse: COMPUTE_WH
    Database: ACCIDENT_DW
    Schema: MART

Recommended charts:

- Crashes by month / year  
- Severity distribution (minor, moderate, severe, fatal)  
- Collision type breakdown  
- Weather vs. traffic control heatmap  
- Road surface and lighting condition impacts  
- Geospatial accident hotspots (map)  
- Simple crash forecast over time  

Combine these into a single **Accident Analytics Dashboard** for your presentation.

---

## ğŸ“„ License

For educational use in **DATA 226 â€” Data Warehousing** (San JosÃ© State University).
